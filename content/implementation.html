<!DOCTYPE html>
<head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="keywords" content="hudi, index, storage, compaction, cleaning, implementation">
<title>Implementation | Hudi</title>
<link rel="stylesheet" href="css/syntax.css">


<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<!--<link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">-->
<link rel="stylesheet" href="css/modern-business.css">
<link rel="stylesheet" href="css/lavish-bootstrap.css">
<link rel="stylesheet" href="css/customstyles.css">
<link rel="stylesheet" href="css/theme-blue.css">

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="js/jquery.navgoco.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/2.0.0/anchor.min.js"></script>
<script src="js/toc.js"></script>
<script src="js/customscripts.js"></script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-93561550-1', 'auto');
  ga('send', 'pageview');

</script>

<link rel="shortcut icon" href="images/favicon.ico">

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link rel="alternate" type="application/rss+xml" title="" href="http://0.0.0.0:4000feed.xml">

    <script>
        $(document).ready(function() {
            // Initialize navgoco with default options
            $("#mysidebar").navgoco({
                caretHtml: '',
                accordion: true,
                openClass: 'active', // open
                save: false, // leave false or nav highlighting doesn't work right
                cookie: {
                    name: 'navgoco',
                    expires: false,
                    path: '/'
                },
                slide: {
                    duration: 400,
                    easing: 'swing'
                }
            });

            $("#collapseAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', false);
            });

            $("#expandAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', true);
            });

        });

    </script>
    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>
    

</head>
<body>
<!-- Navigation -->

<nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container topnavlinks">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <a class="fa fa-lg navbar-brand" href="index.html">&nbsp;<span class="projectTitle">
              <img src="images/hudi_site_logo.png" alt="Hudi logo"/>
              <!--Hudi-->
            </span><br/>
            <p class="navbar-incubate">(Incubating)</p></a>
        </div>
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <!-- entries without drop-downs appear here -->
                
                
                
                <li><a href="news">News</a></li>
                
                
                
                <li><a href="community.html">Community</a></li>
                
                
                
                <li><a href="https://github.com/uber/hoodie" target="_blank">Code</a></li>
                
                
                
                <!-- entries with drop-downs appear here -->
                <!-- conditional logic to control which topnav appears for the audience defined in the configuration file.-->
                
                
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">Developers<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        
                        
                        <li><a href="contributing.html">Contributing</a></li>
                        
                        
                        
                        <li><a href="https://cwiki.apache.org/confluence/display/HUDI" target="_blank">Wiki/Designs</a></li>
                        
                        
                        
                        <li><a href="https://issues.apache.org/jira/projects/HUDI/summary" target="_blank">Issues</a></li>
                        
                        
                        
                        <li><a href="https://cwiki.apache.org/confluence/pages/viewrecentblogposts.action?key=HUDI" target="_blank">Blog</a></li>
                        
                        
                        
                        <li><a href="https://projects.apache.org/project.html?incubator-hudi" target="_blank">Team</a></li>
                        
                        
                    </ul>
                </li>
                
                
                
			<li>



  <a class="email" title="Submit feedback" href="#" onclick="javascript:window.location='mailto:dev@hudi.apache.org?subject=Hudi Documentation feedback&body=I have some feedback about the Implementation page: ' + window.location.href;"><i class="fa fa-envelope-o"></i> Feedback</a>

<li>

		
                <!--comment out this block if you want to hide search-->
                <li>
                    <!--start search-->
                    <div id="search-demo-container">
                        <input type="text" id="search-input" placeholder="search...">
                        <ul id="results-container"></ul>
                    </div>
                    <script src="js/jekyll-search.js" type="text/javascript"></script>
                    <script type="text/javascript">
                            SimpleJekyllSearch.init({
                                searchInput: document.getElementById('search-input'),
                                resultsContainer: document.getElementById('results-container'),
                                dataSource: 'search.json',
                                searchResultTemplate: '<li><a href="{url}" title="Implementation">{title}</a></li>',
                    noResultsText: 'No results found.',
                            limit: 10,
                            fuzzy: true,
                    })
                    </script>
                    <!--end search-->
                </li>
            </ul>
        </div>
        </div>
        <!-- /.container -->
</nav>

<!-- Page Content -->
<div class="container">
    <div class="col-lg-12">&nbsp;</div>
    <!-- Content Row -->
    <div class="row">
        <!-- Sidebar Column -->
        <div class="col-md-3">

          








<ul id="mysidebar" class="nav">
    <li class="sidebarTitle">Latest Version</li>
    
    
    
    <li>
        <a href="#">Getting Started</a>
        <ul>
            
            
            
            <li><a href="index.html">Overview</a></li>
            
            
            
            
            
            
            <li><a href="quickstart.html">Quickstart</a></li>
            
            
            
            
            
            
            <li><a href="use_cases.html">Use Cases</a></li>
            
            
            
            
            
            
            <li><a href="powered_by.html">Talks & Powered By</a></li>
            
            
            
            
        </ul>
        
        
    
    <li>
        <a href="#">Documentation</a>
        <ul>
            
            
            
            <li><a href="concepts.html">Concepts</a></li>
            
            
            
            
            
            
            <li class="active"><a href="implementation.html">Implementation</a></li>
            
            
            
            
            
            
            <li><a href="configurations.html">Configurations</a></li>
            
            
            
            
            
            
            <li><a href="sql_queries.html">SQL Queries</a></li>
            
            
            
            
            
            
            <li><a href="migration_guide.html">Migration Guide</a></li>
            
            
            
            
            
            
            <li><a href="incremental_processing.html">Incremental Processing</a></li>
            
            
            
            
            
            
            <li><a href="admin_guide.html">Admin Guide</a></li>
            
            
            
            
            
            
            <li><a href="comparison.html">Comparison</a></li>
            
            
            
            
        </ul>
        
        
        
        <!-- if you aren't using the accordion, uncomment this block:
           <p class="external">
               <a href="#" id="collapseAll">Collapse All</a> | <a href="#" id="expandAll">Expand All</a>
           </p>
           -->
    </li>
</ul>
</div>

<!-- this highlights the active parent class in the navgoco sidebar. this is critical so that the parent expands when you're viewing a page. This must appear below the sidebar code above. Otherwise, if placed inside customscripts.js, the script runs before the sidebar code runs and the class never gets inserted.-->
<script>$("li.active").parents('li').toggleClass("active");</script>

    <!-- Content Column -->
    <div class="col-md-9">
        <div class="post-header">
   <h1 class="post-title-main">Implementation</h1>
</div>



<div class="post-content">

   

    


    

  <p>Hudi (pronounced “Hoodie”) is implemented as a Spark library, which makes it easy to integrate into existing data pipelines or ingestion
libraries (which we will refer to as <code class="highlighter-rouge">Hudi clients</code>). Hudi Clients prepare an <code class="highlighter-rouge">RDD[HoodieRecord]</code> that contains the data to be upserted and
Hudi upsert/insert is merely a Spark DAG, that can be broken into two big pieces.</p>

<ul>
  <li>
    <p><strong>Indexing</strong> :  A big part of Hudi’s efficiency comes from indexing the mapping from record keys to the file ids, to which they belong to.
 This index also helps the <code class="highlighter-rouge">HoodieWriteClient</code> separate upserted records into inserts and updates, so they can be treated differently.
 <code class="highlighter-rouge">HoodieReadClient</code> supports operations such as <code class="highlighter-rouge">filterExists</code> (used for de-duplication of table) and an efficient batch <code class="highlighter-rouge">read(keys)</code> api, that
 can read out the records corresponding to the keys using the index much quickly, than a typical scan via a query. The index is also atomically
 updated each commit, and is also rolled back when commits are rolled back.</p>
  </li>
  <li>
    <p><strong>Storage</strong> : The storage part of the DAG is responsible for taking an <code class="highlighter-rouge">RDD[HoodieRecord]</code>, that has been tagged as
 an insert or update via index lookup, and writing it out efficiently onto storage.</p>
  </li>
</ul>

<h2 id="index">Index</h2>

<p>Hudi currently provides two choices for indexes : <code class="highlighter-rouge">BloomIndex</code> and <code class="highlighter-rouge">HBaseIndex</code> to map a record key into the file id to which it belongs to. This enables
us to speed up upserts significantly, without scanning over every record in the dataset. Hudi Indices can be classified based on
their ability to lookup records across partition. A <code class="highlighter-rouge">global</code> index does not need partition information for finding the file-id for a record key
but a <code class="highlighter-rouge">non-global</code> does.</p>

<h4 id="hbase-index-global">HBase Index (global)</h4>

<p>Here, we just use HBase in a straightforward way to store the mapping above. The challenge with using HBase (or any external key-value store
 for that matter) is performing rollback of a commit and handling partial index updates.
 Since the HBase table is indexed by record key and not commit Time, we would have to scan all the entries which will be prohibitively expensive.
 Insteead, we store the commit time with the value and discard its value if it does not belong to a valid commit.</p>

<h4 id="bloom-index-non-global">Bloom Index (non-global)</h4>

<p>This index is built by adding bloom filters with a very high false positive tolerance (e.g: 1/10^9), to the parquet file footers.
The advantage of this index over HBase is the obvious removal of a big external dependency, and also nicer handling of rollbacks &amp; partial updates
since the index is part of the data file itself.</p>

<p>At runtime, checking the Bloom Index for a given set of record keys effectively amounts to checking all the bloom filters within a given
partition, against the incoming records, using a Spark join. Much of the engineering effort towards the Bloom index has gone into scaling this join
by caching the incoming RDD[HoodieRecord] and dynamically tuning join parallelism, to avoid hitting Spark limitations like 2GB maximum
for partition size. As a result, Bloom Index implementation has been able to handle single upserts upto 5TB, in a reliable manner.</p>

<h2 id="storage">Storage</h2>

<p>The implementation specifics of the two storage types, introduced in <a href="concepts.html">concepts</a> section, are detailed below.</p>

<h4 id="copy-on-write">Copy On Write</h4>

<p>The Spark DAG for this storage, is relatively simpler. The key goal here is to group the tagged Hudi record RDD, into a series of
updates and inserts, by using a partitioner. To achieve the goals of maintaining file sizes, we first sample the input to obtain a <code class="highlighter-rouge">workload profile</code>
that understands the spread of inserts vs updates, their distribution among the partitions etc. With this information, we bin-pack the
records such that</p>

<ul>
  <li>For updates, the latest version of the that file id, is rewritten once, with new values for all records that have changed</li>
  <li>For inserts, the records are first packed onto the smallest file in each partition path, until it reaches the configured maximum size.
Any remaining records after that, are again packed into new file id groups, again meeting the size requirements.</li>
</ul>

<p>In this storage, index updation is a no-op, since the bloom filters are already written as a part of committing data.</p>

<p>In the case of Copy-On-Write, a single parquet file constitutes one <code class="highlighter-rouge">file slice</code> which contains one complete version of
the file</p>

<figure><img class="docimage" src="images/hudi_log_format_v2.png" alt="hudi_log_format_v2.png" style="max-width: 1000px" /></figure>

<h4 id="merge-on-read">Merge On Read</h4>

<p>In the Merge-On-Read storage model, there are 2 logical components - one for ingesting data (both inserts/updates) into the dataset
 and another for creating compacted views. The former is hereby referred to as <code class="highlighter-rouge">Writer</code> while the later
 is referred as <code class="highlighter-rouge">Compactor</code>.</p>

<h5 id="merge-on-read-writer">Merge On Read Writer</h5>

<p>At a high level, Merge-On-Read Writer goes through same stages as Copy-On-Write writer in ingesting data.
 The key difference here is that updates are appended to latest log (delta) file belonging to the latest file slice
 without merging. For inserts, Hudi supports 2 modes:</p>

<ol>
  <li>Inserts to Log Files - This is done for datasets that have an indexable log files (for eg global index)</li>
  <li>Inserts to parquet files - This is done for datasets that do not have indexable log files, for eg bloom index
embedded in parquer files. Hudi treats writing new records in the same way as inserting to Copy-On-Write files.</li>
</ol>

<p>As in the case of Copy-On-Write, the input tagged records are partitioned such that all upserts destined to
a <code class="highlighter-rouge">file id</code> are grouped together. This upsert-batch is written as one or more log-blocks written to log-files.
Hudi allows clients to control log file sizes (See <a href="../configurations">Storage Configs</a>)</p>

<p>The WriteClient API is same for both Copy-On-Write and Merge-On-Read writers.</p>

<p>With Merge-On-Read, several rounds of data-writes would have resulted in accumulation of one or more log-files.
All these log-files along with base-parquet (if exists) constitute a <code class="highlighter-rouge">file slice</code> which represents one complete version
of the file.</p>

<h4 id="compactor">Compactor</h4>

<p>Realtime Readers will perform in-situ merge of these delta log-files to provide the most recent (committed) view of
the dataset. To keep the query-performance in check and eventually achieve read-optimized performance, Hudi supports
compacting these log-files asynchronously to create read-optimized views.</p>

<p>Asynchronous Compaction involves 2 steps:</p>

<ul>
  <li><code class="highlighter-rouge">Compaction Schedule</code> : Hudi Write Client exposes API to create Compaction plans which contains the list of <code class="highlighter-rouge">file slice</code>
to be compacted atomically in a single compaction commit. Hudi allows pluggable strategies for choosing
file slices for each compaction runs. This step is typically done inline by Writer process as Hudi expects
only one schedule is being generated at a time which allows Hudi to enforce the constraint that pending compaction
plans do not step on each other file-slices. This constraint allows for multiple concurrent <code class="highlighter-rouge">Compactors</code> to run at
the same time. Some of the common strategies used for choosing <code class="highlighter-rouge">file slice</code> for compaction are:
    <ul>
      <li>BoundedIO - Limit the number of file slices chosen for a compaction plan by expected total IO (read + write)
needed to complete compaction run</li>
      <li>Log File Size - Prefer file-slices with larger amounts of delta log data to be merged</li>
      <li>Day Based - Prefer file slice belonging to latest day partitions</li>
    </ul>
  </li>
  <li><code class="highlighter-rouge">Compactor</code> : Hudi provides a separate API in Write Client to execute a compaction plan. The compaction
plan (just like a commit) is identified by a timestamp. Most of the design and implementation complexities for Async
Compaction is for guaranteeing snapshot isolation to readers and writer when
multiple concurrent compactors are running. Typical compactor deployment involves launching a separate
spark application which executes pending compactions when they become available. The core logic of compacting
file slices in the Compactor is very similar to that of merging updates in a Copy-On-Write table. The only
difference being in the case of compaction, there is an additional step of merging the records in delta log-files.</li>
</ul>

<p>Here are the main API to lookup and execute a compaction plan.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>   Main API in HoodieWriteClient for running Compaction:
   /**
    * Performs Compaction corresponding to instant-time
    * @param compactionInstantTime   Compaction Instant Time
    * @return
    * @throws IOException
    */
  public JavaRDD&lt;WriteStatus&gt; compact(String compactionInstantTime) throws IOException;

  To lookup all pending compactions, use the API defined in HoodieReadClient

  /**
   * Return all pending compactions with instant time for clients to decide what to compact next.
   * @return
   */
   public List&lt;Pair&lt;String, HoodieCompactionPlan&gt;&gt; getPendingCompactions();
</code></pre>
</div>
<p>API for scheduling compaction</p>

<div class="highlighter-rouge"><pre class="highlight"><code>
          /**
           * Schedules a new compaction instant
           * @param extraMetadata
           * @return Compaction Instant timestamp if a new compaction plan is scheduled
           */
           Optional&lt;String&gt; scheduleCompaction(Optional&lt;Map&lt;String, String&gt;&gt; extraMetadata) throws IOException;
</code></pre>
</div>

<p>Refer to  <strong>hoodie-client/src/test/java/HoodieClientExample.java</strong> class for an example of how compaction
is scheduled and executed.</p>

<h5 id="deployment-models">Deployment Models</h5>

<p>These are typical Hudi Writer and Compaction deployment models</p>

<ul>
  <li><code class="highlighter-rouge">Inline Compaction</code> : At each round, a single spark application ingests new batch to dataset. It then optionally decides to schedule
   a compaction run and executes it in sequence.</li>
  <li><code class="highlighter-rouge">Single Dedicated Async Compactor</code> :  The Spark application which brings in new changes to dataset (writer) periodically
 schedules compaction. The Writer application does not run compaction inline. A separate spark applications periodically
 probes for pending compaction and executes the compaction.</li>
  <li><code class="highlighter-rouge"> Multi Async Compactors</code> : This mode is similar to <code class="highlighter-rouge">Single Dedicated Async Compactor</code> mode. The main difference being
 now there can be more than one spark application picking different compactions and executing them in parallel.
 In order to ensure compactors do not step on each other, they use coordination service like zookeeper to pickup unique
 pending compaction instants and run them.</li>
</ul>

<p>The Compaction process requires one executor per file-slice in the compaction plan. So, the best resource allocation
strategy (both in terms of speed and resource usage) for clusters supporting dynamic allocation is to lookup the compaction
plan to be run to figure out the number of file slices being compacted and choose that many number of executors.</p>

<h2 id="async-compaction-design-deep-dive-optional">Async Compaction Design Deep-Dive (Optional)</h2>

<p>For the purpose of this section, it is important to distinguish between 2 types of commits as pertaining to the file-group:</p>

<p>A commit which generates a merged and read-optimized file-slice is called <code class="highlighter-rouge">snapshot commit</code> (SC) with respect to that file-group.
A commit which merely appended the new/updated records assigned to the file-group into a new log block is called <code class="highlighter-rouge">delta commit</code> (DC)
with respect to that file-group.</p>

<h3 id="algorithm">Algorithm</h3>

<p>The algorithm is described with an illustration. Let us assume a scenario where there are commits SC1, DC2, DC3 that have
already completed on a data-set. Commit DC4 is currently ongoing with the writer (ingestion) process using it to upsert data.
Let us also imagine there are a set of file-groups (FG1 … FGn) in the data-set whose latest version (<code class="highlighter-rouge">File-Slice</code>)
contains the base file created by commit SC1 (snapshot-commit in columnar format) and a log file containing row-based
log blocks of 2 delta-commits (DC2 and DC3).</p>

<figure><img class="docimage" src="images/async_compac_1.png" alt="async_compac_1.png" style="max-width: 1000px" /></figure>

<ul>
  <li>Writer (Ingestion) that is going to commit “DC4” starts. The record updates in this batch are grouped by file-groups
and appended in row formats to the corresponding log file as delta commit. Let us imagine a subset of file-groups has
this new log block (delta commit) DC4 added.</li>
  <li>
    <p>Before the writer job completes, it runs the compaction strategy to decide which file-group to compact by compactor
and creates a new compaction-request commit SC5. This commit file is marked as “requested” with metadata denoting
which fileIds to compact (based on selection policy). Writer completes without running compaction (will be run async).</p>

    <figure><img class="docimage" src="images/async_compac_2.png" alt="async_compac_2.png" style="max-width: 1000px" /></figure>
  </li>
  <li>
    <p>Writer job runs again ingesting next batch. It starts with commit DC6. It reads the earliest inflight compaction
request marker commit in timeline order and collects the (fileId, Compaction Commit Id “CcId” ) pairs from meta-data.
Ingestion DC6 ensures a new file-slice with base-commit “CcId” gets allocated for the file-group.
The Writer will simply append records in row-format to the first log-file (as delta-commit) assuming the
base-file (“Phantom-Base-File”) will be created eventually by the compactor.</p>

    <figure><img class="docimage" src="images/async_compac_3.png" alt="async_compac_3.png" style="max-width: 1000px" /></figure>
  </li>
  <li>
    <p>Compactor runs at some time  and commits at “Tc” (concurrently or before/after Ingestion DC6). It reads the commit-timeline
and finds the first unprocessed compaction request marker commit. Compactor reads the commit’s metadata finding the
file-slices to be compacted. It compacts the file-slice and creates the missing base-file (“Phantom-Base-File”)
with “CCId” as the commit-timestamp. Compactor then marks the compaction commit timestamp as completed.
It is important to realize that at data-set level, there could be different file-groups requesting compaction at
different commit timestamps.</p>

    <figure><img class="docimage" src="images/async_compac_4.png" alt="async_compac_4.png" style="max-width: 1000px" /></figure>
  </li>
  <li>Near Real-time reader interested in getting the latest snapshot will have 2 cases. Let us assume that the
incremental ingestion (writer at DC6) happened before the compaction (some time “Tc”’).<br />
The below description is with regards to compaction from file-group perspective.
    <ul>
      <li><code class="highlighter-rouge">Reader querying at time between ingestion completion time for DC6 and compaction finish “Tc”</code>:
Hudi’s implementation will be changed to become aware of file-groups currently waiting for compaction and
merge log-files corresponding to DC2-DC6 with the base-file corresponding to SC1. In essence, Hudi will create
a pseudo file-slice by combining the 2 file-slices starting at base-commits SC1 and SC5 to one.
For file-groups not waiting for compaction, the reader behavior is essentially the same - read latest file-slice
and merge on the fly.</li>
      <li><code class="highlighter-rouge">Reader querying at time after compaction finished (&gt; “Tc”)</code> : In this case, reader will not find any pending
compactions in the timeline and will simply have the current behavior of reading the latest file-slice and
merging on-the-fly.</li>
    </ul>
  </li>
  <li>Read-Optimized View readers will query against the latest columnar base-file for each file-groups.</li>
</ul>

<p>The above algorithm explains Async compaction w.r.t a single compaction run on a single file-group. It is important
to note that multiple compaction plans can be run concurrently as they are essentially operating on different
file-groups.</p>

<h2 id="performance">Performance</h2>

<p>In this section, we go over some real world performance numbers for Hudi upserts, incremental pull and compare them against
the conventional alternatives for achieving these tasks.</p>

<h4 id="upsert-vs-bulk-loading">Upsert vs Bulk Loading</h4>

<p>Following shows the speed up obtained for NoSQL ingestion, by switching from bulk loads off HBase to Parquet to incrementally upserting
on a Hudi dataset, on 5 tables ranging from small to huge.</p>

<figure><img class="docimage" src="images/hudi_upsert_perf1.png" alt="hudi_upsert_perf1.png" style="max-width: 1000px" /></figure>

<p>Given Hudi can build the dataset incrementally, it opens doors for also scheduling ingesting more frequently thus reducing latency, with
significant savings on the overall compute cost.</p>

<figure><img class="docimage" src="images/hudi_upsert_perf2.png" alt="hudi_upsert_perf2.png" style="max-width: 1000px" /></figure>

<p>Hudi upserts have been stress tested upto 4TB in a single commit across the t1 table.</p>

<h4 id="copy-on-write-regular-query-performance">Copy On Write Regular Query Performance</h4>

<p>The major design goal for copy-on-write storage was to achieve the latency reduction &amp; efficiency gains in previous section,
with no impact on queries. Following charts compare the Hudi vs non-Hudi datasets across Hive/Presto/Spark queries.</p>

<p><strong>Hive</strong></p>

<figure><img class="docimage" src="images/hudi_query_perf_hive.png" alt="hudi_query_perf_hive.png" style="max-width: 800px" /></figure>

<p><strong>Spark</strong></p>

<figure><img class="docimage" src="images/hudi_query_perf_spark.png" alt="hudi_query_perf_spark.png" style="max-width: 1000px" /></figure>

<p><strong>Presto</strong></p>

<figure><img class="docimage" src="images/hudi_query_perf_presto.png" alt="hudi_query_perf_presto.png" style="max-width: 1000px" /></figure>



    <div class="tags">
        
    </div>

    

</div>

<hr class="shaded"/>

<footer>
            <div class="row">
                <div class="col-lg-12 footer">
                  <p>
                  Copyright &copy; <span id="copyright-year">2019</span> <a href="https://apache.org">The Apache Software Foundation</a>,
                  Licensed under the Apache License, Version 2.0<br>
                  Apache and the Apache feather logo are trademarks of The Apache Software Foundation.| <a href="/privacy">Privacy Policy</a><br>
                  <a class="footer-link-img" href="https://apache.org">
                    <img src="images/asf_logo.svg" alt="The Apache Software Foundation" height="100px" widh="50px"></a>
                  </p>
                  <p>
                  Apache Hudi is an effort undergoing incubation at The Apache Software Foundation (ASF), sponsored by the <a href="http://incubator.apache.org/">Apache Incubator</a>.
                  Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have
                  stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a
                  reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
                  </p>
                </div>
            </div>
</footer>


    </div>
    <!-- /.row -->
</div>
<!-- /.container -->
    </div>

</body>

</html>