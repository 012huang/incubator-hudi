<!DOCTYPE html>
<head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="In this page, we will discuss some available tools for ingesting data incrementally & consuming the changes.">
<meta name="keywords" content="hudi, incremental, batch, stream, processing, Hive, ETL, Spark SQL">
<title>Incremental Processing | Hudi</title>
<link rel="stylesheet" href="css/syntax.css">


<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<!--<link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">-->
<link rel="stylesheet" href="css/modern-business.css">
<link rel="stylesheet" href="css/lavish-bootstrap.css">
<link rel="stylesheet" href="css/customstyles.css">
<link rel="stylesheet" href="css/theme-blue.css">

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="js/jquery.navgoco.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/2.0.0/anchor.min.js"></script>
<script src="js/toc.js"></script>
<script src="js/customscripts.js"></script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-93561550-1', 'auto');
  ga('send', 'pageview');

</script>

<link rel="shortcut icon" href="images/favicon.ico">

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link rel="alternate" type="application/rss+xml" title="" href="http://0.0.0.0:4000feed.xml">

    <script>
        $(document).ready(function() {
            // Initialize navgoco with default options
            $("#mysidebar").navgoco({
                caretHtml: '',
                accordion: true,
                openClass: 'active', // open
                save: false, // leave false or nav highlighting doesn't work right
                cookie: {
                    name: 'navgoco',
                    expires: false,
                    path: '/'
                },
                slide: {
                    duration: 400,
                    easing: 'swing'
                }
            });

            $("#collapseAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', false);
            });

            $("#expandAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', true);
            });

        });

    </script>
    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>
    

</head>
<body>
<!-- Navigation -->

<nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container topnavlinks">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <a class="fa fa-lg navbar-brand" href="index.html">&nbsp;<span class="projectTitle">
              <img src="images/hudi_site_logo.png" alt="Hudi logo"/>
              <!--Hudi-->
            </span><br/>
            <p class="navbar-incubate">(Incubating)</p></a>
        </div>
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <!-- entries without drop-downs appear here -->
                
                
                
                <li><a href="news">News</a></li>
                
                
                
                <li><a href="community.html">Community</a></li>
                
                
                
                <li><a href="https://github.com/uber/hoodie" target="_blank">Code</a></li>
                
                
                
                <!-- entries with drop-downs appear here -->
                <!-- conditional logic to control which topnav appears for the audience defined in the configuration file.-->
                
                
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">Developers<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        
                        
                        <li><a href="contributing.html">Contributing</a></li>
                        
                        
                        
                        <li><a href="https://cwiki.apache.org/confluence/display/HUDI" target="_blank">Wiki/Designs</a></li>
                        
                        
                        
                        <li><a href="https://issues.apache.org/jira/projects/HUDI/summary" target="_blank">Issues</a></li>
                        
                        
                        
                        <li><a href="https://cwiki.apache.org/confluence/pages/viewrecentblogposts.action?key=HUDI" target="_blank">Blog</a></li>
                        
                        
                        
                        <li><a href="https://projects.apache.org/project.html?incubator-hudi" target="_blank">Team</a></li>
                        
                        
                    </ul>
                </li>
                
                
                
			<li>



  <a class="email" title="Submit feedback" href="#" onclick="javascript:window.location='mailto:dev@hudi.apache.org?subject=Hudi Documentation feedback&body=I have some feedback about the Incremental Processing page: ' + window.location.href;"><i class="fa fa-envelope-o"></i> Feedback</a>

<li>

		
                <!--comment out this block if you want to hide search-->
                <li>
                    <!--start search-->
                    <div id="search-demo-container">
                        <input type="text" id="search-input" placeholder="search...">
                        <ul id="results-container"></ul>
                    </div>
                    <script src="js/jekyll-search.js" type="text/javascript"></script>
                    <script type="text/javascript">
                            SimpleJekyllSearch.init({
                                searchInput: document.getElementById('search-input'),
                                resultsContainer: document.getElementById('results-container'),
                                dataSource: 'search.json',
                                searchResultTemplate: '<li><a href="{url}" title="Incremental Processing">{title}</a></li>',
                    noResultsText: 'No results found.',
                            limit: 10,
                            fuzzy: true,
                    })
                    </script>
                    <!--end search-->
                </li>
            </ul>
        </div>
        </div>
        <!-- /.container -->
</nav>

<!-- Page Content -->
<div class="container">
    <div class="col-lg-12">&nbsp;</div>
    <!-- Content Row -->
    <div class="row">
        <!-- Sidebar Column -->
        <div class="col-md-3">

          








<ul id="mysidebar" class="nav">
    <li class="sidebarTitle">Latest Version</li>
    
    
    
    <li>
        <a href="#">Getting Started</a>
        <ul>
            
            
            
            <li><a href="index.html">Overview</a></li>
            
            
            
            
            
            
            <li><a href="quickstart.html">Quickstart</a></li>
            
            
            
            
            
            
            <li><a href="use_cases.html">Use Cases</a></li>
            
            
            
            
            
            
            <li><a href="powered_by.html">Talks & Powered By</a></li>
            
            
            
            
        </ul>
        
        
    
    <li>
        <a href="#">Documentation</a>
        <ul>
            
            
            
            <li><a href="concepts.html">Concepts</a></li>
            
            
            
            
            
            
            <li><a href="implementation.html">Implementation</a></li>
            
            
            
            
            
            
            <li><a href="configurations.html">Configurations</a></li>
            
            
            
            
            
            
            <li><a href="sql_queries.html">SQL Queries</a></li>
            
            
            
            
            
            
            <li><a href="migration_guide.html">Migration Guide</a></li>
            
            
            
            
            
            
            <li class="active"><a href="incremental_processing.html">Incremental Processing</a></li>
            
            
            
            
            
            
            <li><a href="admin_guide.html">Admin Guide</a></li>
            
            
            
            
            
            
            <li><a href="comparison.html">Comparison</a></li>
            
            
            
            
        </ul>
        
        
        
        <!-- if you aren't using the accordion, uncomment this block:
           <p class="external">
               <a href="#" id="collapseAll">Collapse All</a> | <a href="#" id="expandAll">Expand All</a>
           </p>
           -->
    </li>
</ul>
</div>

<!-- this highlights the active parent class in the navgoco sidebar. this is critical so that the parent expands when you're viewing a page. This must appear below the sidebar code above. Otherwise, if placed inside customscripts.js, the script runs before the sidebar code runs and the class never gets inserted.-->
<script>$("li.active").parents('li').toggleClass("active");</script>

    <!-- Content Column -->
    <div class="col-md-9">
        <div class="post-header">
   <h1 class="post-title-main">Incremental Processing</h1>
</div>



<div class="post-content">

   
    <div class="summary">In this page, we will discuss some available tools for ingesting data incrementally & consuming the changes.</div>
   

    


    

  <p>As discussed in the concepts section, the two basic primitives needed for <a href="https://www.oreilly.com/ideas/ubers-case-for-incremental-processing-on-hadoop">incrementally processing</a>,
data using Hudi are <code class="highlighter-rouge">upserts</code> (to apply changes to a dataset) and <code class="highlighter-rouge">incremental pulls</code> (to obtain a change stream/log from a dataset). This section
discusses a few tools that can be used to achieve these on different contexts.</p>

<h2 id="incremental-ingestion">Incremental Ingestion</h2>

<p>Following means can be used to apply a delta or an incremental change to a Hudi dataset. For e.g, the incremental changes could be from a Kafka topic or files uploaded to DFS or
even changes pulled from another Hudi dataset.</p>

<h4 id="deltastreamer-tool">DeltaStreamer Tool</h4>

<p>The <code class="highlighter-rouge">HoodieDeltaStreamer</code> utility provides the way to achieve all of these, by using the capabilities of <code class="highlighter-rouge">HoodieWriteClient</code>, and support simply row-row ingestion (no transformations)
from different sources such as DFS or Kafka.</p>

<p>The tool is a spark job (part of hoodie-utilities), that provides the following functionality</p>

<ul>
  <li>Ability to consume new events from Kafka, incremental imports from Sqoop or output of <code class="highlighter-rouge">HiveIncrementalPuller</code> or files under a folder on DFS</li>
  <li>Support json, avro or a custom payload types for the incoming data</li>
  <li>Pick up avro schemas from DFS or Confluent <a href="https://github.com/confluentinc/schema-registry">schema registry</a>.</li>
  <li>New data is written to a Hudi dataset, with support for checkpointing and registered onto Hive</li>
</ul>

<p>Command line options describe capabilities in more detail (first build hoodie-utilities using <code class="highlighter-rouge">mvn clean package</code>).</p>

<div class="highlighter-rouge"><pre class="highlight"><code>[hoodie]$ spark-submit --class com.uber.hoodie.utilities.deltastreamer.HoodieDeltaStreamer `ls hoodie-utilities/target/hoodie-utilities-*-SNAPSHOT.jar` --help
Usage: &lt;main class&gt; [options]
  Options:
    --help, -h

    --key-generator-class
      Subclass of com.uber.hoodie.KeyGenerator to generate a HoodieKey from
      the given avro record. Built in: SimpleKeyGenerator (uses provided field
      names as recordkey &amp; partitionpath. Nested fields specified via dot
      notation, e.g: a.b.c)
      Default: com.uber.hoodie.SimpleKeyGenerator
    --op
      Takes one of these values : UPSERT (default), INSERT (use when input is
      purely new data/inserts to gain speed)
      Default: UPSERT
      Possible Values: [UPSERT, INSERT, BULK_INSERT]
    --payload-class
      subclass of HoodieRecordPayload, that works off a GenericRecord.
      Implement your own, if you want to do something other than overwriting
      existing value
      Default: com.uber.hoodie.OverwriteWithLatestAvroPayload
    --props
      path to properties file on localfs or dfs, with configurations for
      Hudi client, schema provider, key generator and data source. For
      Hudi client props, sane defaults are used, but recommend use to
      provide basic things like metrics endpoints, hive configs etc. For
      sources, referto individual classes, for supported properties.
      Default: file:///Users/vinoth/bin/hoodie/src/test/resources/delta-streamer-config/dfs-source.properties
    --schemaprovider-class
      subclass of com.uber.hoodie.utilities.schema.SchemaProvider to attach
      schemas to input &amp; target table data, built in options:
      FilebasedSchemaProvider
      Default: com.uber.hoodie.utilities.schema.FilebasedSchemaProvider
    --source-class
      Subclass of com.uber.hoodie.utilities.sources to read data. Built-in
      options: com.uber.hoodie.utilities.sources.{JsonDFSSource (default),
      AvroDFSSource, JsonKafkaSource, AvroKafkaSource, HiveIncrPullSource}
      Default: com.uber.hoodie.utilities.sources.JsonDFSSource
    --source-limit
      Maximum amount of data to read from source. Default: No limit For e.g:
      DFSSource =&gt; max bytes to read, KafkaSource =&gt; max events to read
      Default: 9223372036854775807
    --source-ordering-field
      Field within source record to decide how to break ties between records
      with same key in input data. Default: 'ts' holding unix timestamp of
      record
      Default: ts
    --spark-master
      spark master to use.
      Default: local[2]
  * --target-base-path
      base path for the target Hudi dataset. (Will be created if did not
      exist first time around. If exists, expected to be a Hudi dataset)
  * --target-table
      name of the target table in Hive
    --transformer-class
      subclass of com.uber.hoodie.utilities.transform.Transformer. UDF to
      transform raw source dataset to a target dataset (conforming to target
      schema) before writing. Default : Not set. E:g -
      com.uber.hoodie.utilities.transform.SqlQueryBasedTransformer (which
      allows a SQL query template to be passed as a transformation function)

</code></pre>
</div>

<p>The tool takes a hierarchically composed property file and has pluggable interfaces for extracting data, key generation and providing schema. Sample configs for ingesting from kafka and dfs are
provided under <code class="highlighter-rouge">hoodie-utilities/src/test/resources/delta-streamer-config</code>.</p>

<p>For e.g: once you have Confluent Kafka, Schema registry up &amp; running, produce some test data using (<a href="https://docs.confluent.io/current/ksql/docs/tutorials/generate-custom-test-data.html">impressions.avro</a> provided by schema-registry repo)</p>

<div class="highlighter-rouge"><pre class="highlight"><code>[confluent-5.0.0]$ bin/ksql-datagen schema=../impressions.avro format=avro topic=impressions key=impressionid
</code></pre>
</div>

<p>and then ingest it as follows.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>[hoodie]$ spark-submit --class com.uber.hoodie.utilities.deltastreamer.HoodieDeltaStreamer `ls hoodie-utilities/target/hoodie-utilities-*-SNAPSHOT.jar` \
  --props file://${PWD}/hoodie-utilities/src/test/resources/delta-streamer-config/kafka-source.properties \
  --schemaprovider-class com.uber.hoodie.utilities.schema.SchemaRegistryProvider \
  --source-class com.uber.hoodie.utilities.sources.AvroKafkaSource \
  --source-ordering-field impresssiontime \
  --target-base-path file:///tmp/hoodie-deltastreamer-op --target-table uber.impressions \
  --op BULK_INSERT
</code></pre>
</div>

<p>In some cases, you may want to convert your existing dataset into Hudi, before you can begin ingesting new data. This can be accomplished using the <code class="highlighter-rouge">hdfsparquetimport</code> command on the <code class="highlighter-rouge">hoodie-cli</code>.
Currently, there is support for converting parquet datasets.</p>

<h4 id="via-custom-spark-job">Via Custom Spark Job</h4>

<p>The <code class="highlighter-rouge">hoodie-spark</code> module offers the DataSource API to write any data frame into a Hudi dataset. Following is how we can upsert a dataframe, while specifying the field names that need to be used
for <code class="highlighter-rouge">recordKey =&gt; _row_key</code>, <code class="highlighter-rouge">partitionPath =&gt; partition</code> and <code class="highlighter-rouge">precombineKey =&gt; timestamp</code></p>

<div class="highlighter-rouge"><pre class="highlight"><code>inputDF.write()
       .format("com.uber.hoodie")
       .options(clientOpts) // any of the Hudi client opts can be passed in as well
       .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), "_row_key")
       .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), "partition")
       .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), "timestamp")
       .option(HoodieWriteConfig.TABLE_NAME, tableName)
       .mode(SaveMode.Append)
       .save(basePath);
</code></pre>
</div>

<p>Please refer to <a href="configurations.html">configurations</a> section, to view all datasource options.</p>

<h4 id="syncing-to-hive">Syncing to Hive</h4>

<p>Once new data is written to a Hudi dataset, via tools like above, we need the ability to sync with Hive and reflect the table schema such that queries can pick up new columns and partitions. To do this, Hudi provides a <code class="highlighter-rouge">HiveSyncTool</code>, which can be
invoked as below, once you have built the hoodie-hive module.</p>

<div class="highlighter-rouge"><pre class="highlight"><code> [hoodie-hive]$ java -cp target/hoodie-hive-0.3.6-SNAPSHOT-jar-with-dependencies.jar:target/jars/* com.uber.hoodie.hive.HiveSyncTool --help
Usage: &lt;main class&gt; [options]
  Options:
  * --base-path
       Basepath of Hudi dataset to sync
  * --database
       name of the target database in Hive
    --help, -h

       Default: false
  * --jdbc-url
       Hive jdbc connect url
  * --pass
       Hive password
  * --table
       name of the target table in Hive
  * --user
       Hive username


</code></pre>
</div>

<h2 id="incrementally-pulling">Incrementally Pulling</h2>

<p>Hudi datasets can be pulled incrementally, which means you can get ALL and ONLY the updated &amp; new rows since a specified commit timestamp.
This, together with upserts, are particularly useful for building data pipelines where 1 or more source Hudi tables are incrementally pulled (streams/facts),
joined with other tables (datasets/dimensions), to produce deltas to a target Hudi dataset. Then, using the delta streamer tool these deltas can be upserted into the
target Hudi dataset to complete the pipeline.</p>

<h4 id="via-spark-job">Via Spark Job</h4>
<p>The <code class="highlighter-rouge">hoodie-spark</code> module offers the DataSource API, offers a more elegant way to pull data from Hudi dataset (plus more) and process it via Spark.
This class can be used within existing Spark jobs and offers the following functionality.</p>

<p>A sample incremental pull, that will obtain all records written since <code class="highlighter-rouge">beginInstantTime</code>, looks like below.</p>

<div class="highlighter-rouge"><pre class="highlight"><code> Dataset&lt;Row&gt; hoodieIncViewDF = spark.read()
     .format("com.uber.hoodie")
     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY(),
             DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL())
     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY(),
            &lt;beginInstantTime&gt;)
     .load(tablePath); // For incremental view, pass in the root/base path of dataset
</code></pre>
</div>

<p>Please refer to <a href="configurations.html">configurations</a> section, to view all datasource options.</p>

<p>Additionally, <code class="highlighter-rouge">HoodieReadClient</code> offers the following functionality using Hudi’s implicit indexing.</p>

<table>
  <tbody>
    <tr>
      <td><strong>API</strong></td>
      <td><strong>Description</strong></td>
    </tr>
    <tr>
      <td>read(keys)</td>
      <td>Read out the data corresponding to the keys as a DataFrame, using Hudi’s own index for faster lookup</td>
    </tr>
    <tr>
      <td>filterExists()</td>
      <td>Filter out already existing records from the provided RDD[HoodieRecord]. Useful for de-duplication</td>
    </tr>
    <tr>
      <td>checkExists(keys)</td>
      <td>Check if the provided keys exist in a Hudi dataset</td>
    </tr>
  </tbody>
</table>

<h4 id="hiveincrementalpuller-tool">HiveIncrementalPuller Tool</h4>
<p><code class="highlighter-rouge">HiveIncrementalPuller</code> allows the above to be done via HiveQL, combining the benefits of Hive (reliably process complex SQL queries) and incremental primitives
(speed up query by pulling tables incrementally instead of scanning fully). The tool uses Hive JDBC to run the Hive query saving its results in a temp table.
that can later be upserted. Upsert utility (<code class="highlighter-rouge">HoodieDeltaStreamer</code>) has all the state it needs from the directory structure to know what should be the commit time on the target table.
e.g: <code class="highlighter-rouge">/app/incremental-hql/intermediate/{source_table_name}_temp/{last_commit_included}</code>.The Delta Hive table registered will be of the form <code class="highlighter-rouge"><span class="p">{</span><span class="err">tmpdb</span><span class="p">}</span><span class="err">.</span><span class="p">{</span><span class="err">source_table</span><span class="p">}</span><span class="err">_</span><span class="p">{</span><span class="err">last_commit_included</span><span class="p">}</span></code>.</p>

<p>The following are the configuration options for HiveIncrementalPuller</p>

<table>
  <tbody>
    <tr>
      <td><strong>Config</strong></td>
      <td><strong>Description</strong></td>
      <td><strong>Default</strong></td>
    </tr>
    <tr>
      <td>hiveUrl</td>
      <td>Hive Server 2 URL to connect to</td>
      <td> </td>
    </tr>
    <tr>
      <td>hiveUser</td>
      <td>Hive Server 2 Username</td>
      <td> </td>
    </tr>
    <tr>
      <td>hivePass</td>
      <td>Hive Server 2 Password</td>
      <td> </td>
    </tr>
    <tr>
      <td>queue</td>
      <td>YARN Queue name</td>
      <td> </td>
    </tr>
    <tr>
      <td>tmp</td>
      <td>Directory where the temporary delta data is stored in DFS. The directory structure will follow conventions. Please see the below section.</td>
      <td> </td>
    </tr>
    <tr>
      <td>extractSQLFile</td>
      <td>The SQL to execute on the source table to extract the data. The data extracted will be all the rows that changed since a particular point in time.</td>
      <td> </td>
    </tr>
    <tr>
      <td>sourceTable</td>
      <td>Source Table Name. Needed to set hive environment properties.</td>
      <td> </td>
    </tr>
    <tr>
      <td>targetTable</td>
      <td>Target Table Name. Needed for the intermediate storage directory structure.</td>
      <td> </td>
    </tr>
    <tr>
      <td>sourceDataPath</td>
      <td>Source DFS Base Path. This is where the Hudi metadata will be read.</td>
      <td> </td>
    </tr>
    <tr>
      <td>targetDataPath</td>
      <td>Target DFS Base path. This is needed to compute the fromCommitTime. This is not needed if fromCommitTime is specified explicitly.</td>
      <td> </td>
    </tr>
    <tr>
      <td>tmpdb</td>
      <td>The database to which the intermediate temp delta table will be created</td>
      <td>hoodie_temp</td>
    </tr>
    <tr>
      <td>fromCommitTime</td>
      <td>This is the most important parameter. This is the point in time from which the changed records are pulled from.</td>
      <td> </td>
    </tr>
    <tr>
      <td>maxCommits</td>
      <td>Number of commits to include in the pull. Setting this to -1 will include all the commits from fromCommitTime. Setting this to a value &gt; 0, will include records that ONLY changed in the specified number of commits after fromCommitTime. This may be needed if you need to catch up say 2 commits at a time.</td>
      <td>3</td>
    </tr>
    <tr>
      <td>help</td>
      <td>Utility Help</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>Setting the fromCommitTime=0 and maxCommits=-1 will pull in the entire source dataset and can be used to initiate backfills. If the target dataset is a Hudi dataset,
then the utility can determine if the target dataset has no commits or is behind more than 24 hour (this is configurable),
it will automatically use the backfill configuration, since applying the last 24 hours incrementally could take more time than doing a backfill. The current limitation of the tool
is the lack of support for self-joining the same table in mixed mode (normal and incremental modes).</p>


    <div class="tags">
        
    </div>

    

</div>

<hr class="shaded"/>

<footer>
            <div class="row">
                <div class="col-lg-12 footer">
                  <p>
                  Copyright &copy; <span id="copyright-year">2019</span> <a href="https://apache.org">The Apache Software Foundation</a>,
                  Licensed under the Apache License, Version 2.0<br>
                  Apache and the Apache feather logo are trademarks of The Apache Software Foundation.| <a href="/privacy">Privacy Policy</a><br>
                  <a class="footer-link-img" href="https://apache.org">
                    <img src="images/asf_logo.svg" alt="The Apache Software Foundation" height="100px" widh="50px"></a>
                  </p>
                  <p>
                  Apache Hudi is an effort undergoing incubation at The Apache Software Foundation (ASF), sponsored by the <a href="http://incubator.apache.org/">Apache Incubator</a>.
                  Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have
                  stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a
                  reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
                  </p>
                </div>
            </div>
</footer>


    </div>
    <!-- /.row -->
</div>
<!-- /.container -->
    </div>

</body>

</html>